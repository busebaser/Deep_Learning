{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **Training an agent in CartPole-v1 environment with Deep Reinforcement Learning Algorithm using Keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque # Used to define the agent's memory\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm # to visualize progress\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQL Agent Class\n",
    "class DQLAgent:\n",
    "    def __init__(self,env):\n",
    "        # Gets the state space size of the CartPole environment.\n",
    "        # In the CartPole environment, the situation is 4-dimensional. (position of the car, speed, angle of the bar and angular velocity).\n",
    "        self.state_size = env.observation_space.shape[0] \n",
    "        \n",
    "        self.action_size = env.action_space.n # move left or move right\n",
    "        \n",
    "        self.gamma = 0.995 # effect of future rewards on current value\n",
    "        # If it is close to 0, the agent prefers short-term rewards.\n",
    "        # If it is closer to 1, the agent considers future rewards more.\n",
    "    \n",
    "        self.learning_rate = 0.001 \n",
    "        \n",
    "        # Epsilon = Exploration rate, probability of agent choosing random action\n",
    "        self.epsilon = 1.0 # discovery rate\n",
    "        self.epsilon_decay = 0.995 # The rate at which epsilon increases in each iteration is that as epsilon decreases, it learns more and discovers less.\n",
    "        self.epsilon_min = 0.01\n",
    "        \n",
    "        # deque : It automatically deletes old data when it reaches a certain capacity.\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        \n",
    "        self.alpha = 0.6 # Prioritization parameter\n",
    "        self.beta = 0.4 # Importance sampling correction parameter \n",
    "        \n",
    "        self.model = self.build_model()\n",
    "        \n",
    "    def build_model(self):\n",
    "        \n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Dense(64, input_dim=self.state_size, activation=\"relu\"))\n",
    "        model.add(Dense(64, activation=\"relu\"))\n",
    "        model.add(Dense(32, activation=\"relu\"))\n",
    "        model.add(Dense(self.action_size, activation=\"linear\"))\n",
    "        \n",
    "        # Compiling\n",
    "        model.compile(loss = \"mse\", optimizer=Adam(learning_rate= self.learning_rate))\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        state = np.array(state, dtype=np.float32).reshape(1, -1)\n",
    "        next_state = np.array(next_state, dtype=np.float32).reshape(1, -1)\n",
    "\n",
    "        # Calculating TD Error\n",
    "        target = reward\n",
    "        if not done:\n",
    "            target += self.gamma * np.amax(self.model.predict(next_state, verbose=0)[0])\n",
    "    \n",
    "        predicted = self.model.predict(state, verbose=0)[0][action]\n",
    "        error = abs(target - predicted)  \n",
    "\n",
    "        self.memory.append((state, action, reward, next_state, done, error))\n",
    "\n",
    "\n",
    "    \n",
    "    def act(self,state, env):\n",
    "        \n",
    "        state = np.array(state, dtype=np.float32).reshape(1, -1)  \n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randint(0, self.action_size - 1)  # Random act\n",
    "        act_values = self.model.predict(state, verbose=0)\n",
    "        return np.argmax(act_values[0])  # Pick the best act\n",
    "        \n",
    "    # Prioritized Experience Replay has been implemented.\n",
    "    # It provides a faster training process by prioritizing experiences with a high error rate.\n",
    "    def replay(self,batch_size): # deep q network is trained by replaying experiences\n",
    "        \n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        # Sample Selection with Priority Probabilities\n",
    "        errors = np.array([exp[5] for exp in self.memory])  # Get errors\n",
    "        probabilities = errors ** self.alpha  # Apply Prioritization \n",
    "        probabilities /= probabilities.sum()  # Normalization\n",
    "\n",
    "        \n",
    "        indices = np.random.choice(len(self.memory), batch_size, p=probabilities)\n",
    "        minibatch = [self.memory[i] for i in indices]\n",
    "\n",
    "        for state, action, reward, next_state, done, _ in minibatch:\n",
    "            state = np.array(state, dtype=np.float32).reshape(1, -1)\n",
    "            next_state = np.array(next_state, dtype=np.float32).reshape(1, -1)\n",
    "            \n",
    "            target = reward\n",
    "            if not done:\n",
    "                target += self.gamma * np.amax(self.model.predict(next_state, verbose=0)[0])\n",
    "\n",
    "\n",
    "            target_f = self.model.predict(state, verbose=0)\n",
    "            target_f[0][action] = target\n",
    "\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "\n",
    "\n",
    "        # When beta is small, the agent learns more important examples.\n",
    "        # As beta grows, the agent begins to learn all examples equally.\n",
    "        self.beta = min(1.0, self.beta + 0.001)\n",
    "            \n",
    "    def adaptiveEGreedy(self): # decrease of epsilon over time, balance of exploration and exploitation\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon = self.epsilon * self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:02<00:46,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode : 0, time : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [01:13<12:45, 42.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode : 1, time : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [01:54<11:51, 41.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode : 2, time : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [04:18<21:59, 82.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode : 3, time : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [05:40<20:34, 82.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode : 4, time : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [06:29<16:30, 70.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode : 5, time : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [07:17<13:44, 63.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode : 6, time : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [08:10<12:00, 60.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode : 7, time : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [09:02<10:32, 57.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode : 8, time : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [10:23<10:47, 64.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode : 9, time : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [10:59<08:25, 56.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode : 10, time : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [19:47<26:37, 199.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode : 11, time : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [20:51<18:29, 158.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode : 12, time : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [22:23<13:51, 138.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode : 13, time : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [24:50<11:44, 140.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode : 14, time : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [26:56<09:05, 136.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode : 15, time : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [29:07<06:44, 134.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode : 16, time : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [30:07<03:44, 112.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode : 17, time : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [31:09<01:37, 97.38s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode : 18, time : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [33:17<00:00, 99.89s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode : 19, time : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# trainİNG the dql agent using the gym environment.\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode = \"human\")\n",
    "agent = DQLAgent(env)\n",
    "\n",
    "batch_size = 32 \n",
    "episodes = 20\n",
    "\n",
    "for e in tqdm(range(episodes)):\n",
    "    \n",
    "    state, _ = env.reset() \n",
    "    state = np.array(state, dtype=np.float32).reshape(1, -1)\n",
    "    \n",
    "    time = 0\n",
    "    \n",
    "    while True:\n",
    "        action = agent.act(state, env) \n",
    "        \n",
    "        # the agent implements the action in the environment\n",
    "        (next_state, reward, done,_,_) = env.step(action)\n",
    "        next_state = np.array(next_state, dtype=np.float32).reshape(1, -1)\n",
    "\n",
    "        \n",
    "        # It records the action performed and the information received from the env as a result of the action.\n",
    "        reward = reward if not done else -10 \n",
    "        agent.remember(state,action,reward, next_state, done)\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "        \n",
    "        agent.replay(batch_size) # Starts replay from experiences\n",
    "        \n",
    "        agent.adaptiveEGreedy()\n",
    "    \n",
    "        if done:\n",
    "            print(f\"\\nEpisode : {e}, time : {time}\")\n",
    "            break\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1\n",
      "Time: 2\n",
      "Time: 3\n",
      "Time: 4\n",
      "Time: 5\n",
      "Time: 6\n",
      "Time: 7\n",
      "Time: 8\n",
      "Time: 9\n",
      "Time: 10\n",
      "Time: 11\n",
      "Time: 12\n",
      "Time: 13\n",
      "Time: 14\n",
      "Time: 15\n",
      "Time: 16\n",
      "Time: 17\n",
      "Time: 18\n",
      "Time: 19\n",
      "Time: 20\n",
      "Time: 21\n",
      "Time: 22\n",
      "Time: 23\n",
      "Time: 24\n",
      "Time: 25\n",
      "Time: 26\n",
      "Time: 27\n",
      "Time: 28\n",
      "Time: 29\n",
      "Time: 30\n",
      "Time: 31\n",
      "Time: 32\n",
      "Time: 33\n",
      "Time: 34\n",
      "Time: 35\n",
      "Time: 36\n",
      "Time: 37\n",
      "Time: 38\n",
      "Time: 39\n",
      "Time: 40\n",
      "Time: 41\n",
      "Time: 42\n",
      "Time: 43\n",
      "Time: 44\n",
      "Time: 45\n",
      "Time: 46\n",
      "Episode Done\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "trained_model = agent\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode = \"human\")\n",
    "state = env.reset()[0]\n",
    "state = np.reshape(state, [1,4])\n",
    "\n",
    "for time_t in range(500):\n",
    "    env.render() # Visually render the environment\n",
    "    \n",
    "    action = trained_model.act(state,env)\n",
    "    \n",
    "    (next_state, reward, done, _,_) = env.step(action)\n",
    "    next_state = np.reshape(next_state, [1,4])\n",
    "    state = next_state\n",
    "    \n",
    "    time_t += 1\n",
    "    \n",
    "    print(f\"Time: {time_t}\")\n",
    "    \n",
    "    if done:\n",
    "        print(\"Episode Done\")\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
